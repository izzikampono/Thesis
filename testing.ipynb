{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from decpomdp import DecPOMDP\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from constant import Constants\n",
    "import gc \n",
    "gc.enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game size :\n",
      "\t|S| = 2 , ['s_small', 's_large']\n",
      "\t|Z| = 4 , [('o_small', 'o_small'), ('o_small', 'o_large'), ('o_large', 'o_small'), ('o_large', 'o_large')]\n",
      "\t|U|  with |U_i| = 2\n",
      "intiial_belief : [0.5 0.5]\n",
      "actions : [('observe', 'observe'), ('observe', 'attack'), ('attack', 'observe'), ('attack', 'attack')]\n"
     ]
    }
   ],
   "source": [
    "#set problem \n",
    "file_name = \"2generals\"\n",
    "planning_horizon = 5\n",
    "num_iterations = 1\n",
    "sota_ = False\n",
    "game_type = \"zerosum\"\n",
    "\n",
    "\n",
    "problem = DecPOMDP(file_name,horizon=planning_horizon)\n",
    "Constants.initialize(problem)\n",
    "constant = Constants.get_instance()\n",
    "from pbvi import PBVI\n",
    "from experimentFunctions import Experiment\n",
    "experiment = Experiment(planning_horizon,problem,algorithm=\"max_plane\")\n",
    "print(f\"game size :\\n\\t|S| = {len(problem.states)} , {problem.states}\")\n",
    "print(f\"\\t|Z| = {problem.num_joint_observations} , {problem.joint_observations}\\n\\t|U|  with |U_i| = {problem.num_actions[0]}\")\n",
    "print(f\"intiial_belief : {problem.b0}\\nactions : {problem.joint_actions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWARD MATRIX OF zerosum 2generals GAME\n",
      "\n",
      "Leader :\n",
      "   s_small  s_large\n",
      "0     -1.0     -1.0\n",
      "1    -10.0    -10.0\n",
      "2    -10.0    -10.0\n",
      "3      5.0    -20.0\n",
      "\n",
      "Follower :\n",
      "   s_small  s_large\n",
      "0      1.0      1.0\n",
      "1     10.0     10.0\n",
      "2     10.0     10.0\n",
      "3     -5.0     20.0\n"
     ]
    }
   ],
   "source": [
    "#see reward matrix of both player\n",
    "print(f\"REWARD MATRIX OF {game_type} {problem.name} GAME\")\n",
    "print(\"\\nLeader :\")\n",
    "print(f\"{pd.DataFrame(constant.REWARDS[game_type][0],columns=problem.states)}\")\n",
    "print(\"\\nFollower :\")\n",
    "print(f\"{pd.DataFrame(constant.REWARDS[game_type][1],columns=problem.states)}\")\n",
    "\n",
    "r = constant.REWARDS[game_type][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA True 5 \n",
      "\tbelief expansion done, belief space size = 11\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 4 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 1! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -2.27720325, -10.        ]), array([0., 3.])] , value =(-6.1386016244092625, 1.5)\n",
      "\treconstructed tabular alpha : [array([ -3.17818318, -10.        ]), array([0., 3.])], value = (-6.589091590914461, 1.5)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x14436fd90>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -1.2772032488185254, from tabular = -2.178183181828923\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 2) :[array([ -1.71436678, -10.        ]), array([0., 3.])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-1.964604021929435, 0.09060402684563758) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 1 at timestep 1! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -3.37650756, -10.        ]), array([0., 3.])] , value =(-3.5765459247063767, 0.09060402684563758)\n",
      "\treconstructed tabular alpha : [array([ -3.91919617, -10.        ]), array([0., 3.])], value = (-4.102844609637166, 0.09060402684563758)  --  belief [0.96979866 0.03020134]  -- DR <decisionRule.DecisionRule object at 0x144354940>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.3765075625000005, from tabular = -2.919196171875001\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -2.43120978, -10.        ]), array([0., 3.])] , tabular : [array([ -2.99949375, -10.        ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-2.4385430043929675, 0.002906629028013683) ,tabular value : (-3.006276374892347, 0.002906629028013683)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -2.43120978, -10.        ]), array([0., 3.])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-2.6597974066238694, 0.09060402684563758) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -2.43120978, -10.        ]), array([0., 3.])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-2.6597974066238694, 0.09060402684563758) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -2.0994239, -10.       ]), array([0., 3.])] , value =(-6.04971195077357, 1.5)\n",
      "\treconstructed tabular alpha : [array([ -3.86936072, -10.        ]), array([0., 3.])], value = (-6.934680357883758, 1.5)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x144325bd0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -1.0994239015471403, from tabular = -2.8693607157675163\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 2) :[array([ -1.47573678, -10.        ]), array([0., 3.])] , tabular : [array([ -3.91919617, -10.        ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-1.733180971790116, 0.09060402684563758) ,tabular value : (-4.102844609637166, 0.09060402684563758)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-6.934680357883758, 1.5)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-6.04971195077357, 1.5)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-6.04971195077357, 1.5)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values,times ,tabular_value , policy= experiment.run_single_experiment(density=0.000001,gametype=\"stackelberg\",limit=1000,sota=True,iterations=num_iterations)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA True 1 \n",
      "\tbelief expansion done, belief space size = 3\n",
      "\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-1.0, -1.0)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-1.0, -1.0)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA True 2 \n",
      "\tbelief expansion done, belief space size = 5\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (0.32125000000000004, 0.32125000000000004)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (0.32125000000000004, 0.32125000000000004)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA True 3 \n",
      "\tbelief expansion done, belief space size = 7\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-0.034993749999999935, -0.034993749999999935)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (0.019353125000000082, 0.019353125000000082)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA True 4 \n",
      "\tbelief expansion done, belief space size = 9\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game cooperative  :: \n",
      "\tReconstructed Max plane alpha:[array([ 2.67259244, -3.12666967]), array([ 2.67259244, -3.12666967])] , value =(-0.22703861718750007, -0.22703861718750007)\n",
      "\treconstructed tabular alpha : [array([ 2.58165784, -3.35584216]), array([ 2.58165784, -3.35584216])], value = (-0.38709215625000004, -0.38709215625000004)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x1443e9990>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = 3.6725924375, from tabular = 3.5816578437500004\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.1266696718750002, from tabular = -2.3558421562500005\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-0.38709215625000004, -0.38709215625000004)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-0.22703861718750007, -0.22703861718750007)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA True 5 \n",
      "\tbelief expansion done, belief space size = 11\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 4 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 1! \n",
      "Game cooperative  :: \n",
      "\tReconstructed Max plane alpha:[array([ 2.67259244, -3.12666967]), array([ 2.67259244, -3.12666967])] , value =(-0.22703861718750007, -0.22703861718750007)\n",
      "\treconstructed tabular alpha : [array([ 2.58165784, -3.35584216]), array([ 2.58165784, -3.35584216])], value = (-0.38709215625000004, -0.38709215625000004)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x144353880>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = 3.6725924375, from tabular = 3.5816578437500004\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.1266696718750002, from tabular = -2.3558421562500005\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 2 at timestep 1! \n",
      "Game cooperative  :: \n",
      "\tReconstructed Max plane alpha:[array([-0.25885994, -3.5728472 ]), array([-0.25885994, -3.5728472 ])] , value =(-3.472760339398071, -3.472760339398071)\n",
      "\treconstructed tabular alpha : [array([-1.4052993 , -3.91919617]), array([-1.4052993 , -3.91919617])], value = (-3.8432731118917793, -3.8432731118917793)  --  belief [0.03020134 0.96979866]  -- DR <decisionRule.DecisionRule object at 0x1443527a0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = 0.7411400625000001, from tabular = -0.405299296875\n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-2.47799375, -2.99949375]), array([-2.47799375, -2.99949375])]\n",
      "\t\t max_plane value :(-2.6269309501417433, -2.6269309501417433) ,tabular value : (-2.9989884809872973, -2.9989884809872973)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.5728472031250003, from tabular = -2.9191961718750004\n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-2.47799375, -2.99949375]), array([-2.47799375, -2.99949375])]\n",
      "\t\t max_plane value :(-2.6269309501417433, -2.6269309501417433) ,tabular value : (-2.9989884809872973, -2.9989884809872973)  \n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game cooperative  :: \n",
      "\tReconstructed Max plane alpha:[array([ 2.67263333, -3.48401884]), array([ 2.67263333, -3.48401884])] , value =(-0.4056927540429687, -0.4056927540429687)\n",
      "\treconstructed tabular alpha : [array([ 2.58088077, -4.05661923]), array([ 2.58088077, -4.05661923])], value = (-0.737869234179688, -0.737869234179688)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x144352560>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = 3.6726333298437503, from tabular = 3.5808807658203126\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.67259244, -3.12666967]), array([ 2.67259244, -3.12666967])] , tabular : [array([-1.4052993 , -3.91919617]), array([-1.4052993 , -3.91919617])]\n",
      "\t\t max_plane value :(-2.951524171927433, -2.951524171927433) ,tabular value : (-3.8432731118917793, -3.8432731118917793)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.4840188379296877, from tabular = -3.0566192341796885\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.67259244, -3.12666967]), array([ 2.67259244, -3.12666967])] , tabular : [array([-1.4052993 , -3.91919617]), array([-1.4052993 , -3.91919617])]\n",
      "\t\t max_plane value :(-2.951524171927433, -2.951524171927433) ,tabular value : (-3.8432731118917793, -3.8432731118917793)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-0.737869234179688, -0.737869234179688)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-0.4056927540429687, -0.4056927540429687)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA False 1 \n",
      "\tbelief expansion done, belief space size = 3\n",
      "\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-1.0, -1.0)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-1.0, -1.0)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA False 2 \n",
      "\tbelief expansion done, belief space size = 5\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (0.32125000000000004, 0.32125000000000004)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (0.32125000000000004, 0.32125000000000004)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA False 3 \n",
      "\tbelief expansion done, belief space size = 7\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-0.034993749999999935, -0.034993749999999935)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (0.019353125000000082, 0.019353125000000082)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA False 4 \n",
      "\tbelief expansion done, belief space size = 9\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game cooperative  :: \n",
      "\tReconstructed Max plane alpha:[array([ 2.67259244, -3.12666967]), array([ 2.67259244, -3.12666967])] , value =(-0.22703861718750007, -0.22703861718750007)\n",
      "\treconstructed tabular alpha : [array([ 2.58165784, -3.35584216]), array([ 2.58165784, -3.35584216])], value = (-0.38709215625000004, -0.38709215625000004)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x14436d3f0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = 3.6725924375, from tabular = 3.5816578437500004\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.1266696718750002, from tabular = -2.3558421562500005\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-0.38709215625000004, -0.38709215625000004)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-0.22703861718750007, -0.22703861718750007)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving cooperative 2generals GAME WITH SOTA False 5 \n",
      "\tbelief expansion done, belief space size = 11\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 4 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 1! \n",
      "Game cooperative  :: \n",
      "\tReconstructed Max plane alpha:[array([ 2.67259244, -3.12666967]), array([ 2.67259244, -3.12666967])] , value =(-0.22703861718750007, -0.22703861718750007)\n",
      "\treconstructed tabular alpha : [array([ 2.58165784, -3.35584216]), array([ 2.58165784, -3.35584216])], value = (-0.38709215625000004, -0.38709215625000004)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x14462c370>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = 3.6725924375, from tabular = 3.5816578437500004\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.1266696718750002, from tabular = -2.3558421562500005\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 2 at timestep 1! \n",
      "Game cooperative  :: \n",
      "\tReconstructed Max plane alpha:[array([-0.25885994, -3.5728472 ]), array([-0.25885994, -3.5728472 ])] , value =(-3.472760339398071, -3.472760339398071)\n",
      "\treconstructed tabular alpha : [array([-1.4052993 , -3.91919617]), array([-1.4052993 , -3.91919617])], value = (-3.8432731118917793, -3.8432731118917793)  --  belief [0.03020134 0.96979866]  -- DR <decisionRule.DecisionRule object at 0x144633c10>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = 0.7411400625000001, from tabular = -0.405299296875\n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-2.47799375, -2.99949375]), array([-2.47799375, -2.99949375])]\n",
      "\t\t max_plane value :(-2.6269309501417433, -2.6269309501417433) ,tabular value : (-2.9989884809872973, -2.9989884809872973)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.5728472031250003, from tabular = -2.9191961718750004\n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-1.3707625, -2.9492625]), array([-1.3707625, -2.9492625])]\n",
      "\t\t max_plane value :(-2.4719157508389267, -2.4719157508389267) ,tabular value : (-2.9015896812080544, -2.9015896812080544)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 0) :[array([ 2.670775  , -2.63206875]), array([ 2.670775  , -2.63206875])] , tabular : [array([-2.47799375, -2.99949375]), array([-2.47799375, -2.99949375])]\n",
      "\t\t max_plane value :(-2.6269309501417433, -2.6269309501417433) ,tabular value : (-2.9989884809872973, -2.9989884809872973)  \n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game cooperative  :: \n",
      "\tReconstructed Max plane alpha:[array([ 2.67263333, -3.48401884]), array([ 2.67263333, -3.48401884])] , value =(-0.4056927540429687, -0.4056927540429687)\n",
      "\treconstructed tabular alpha : [array([ 2.58088077, -4.05661923]), array([ 2.58088077, -4.05661923])], value = (-0.737869234179688, -0.737869234179688)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x1446cc670>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = 3.6726333298437503, from tabular = 3.5808807658203126\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.67259244, -3.12666967]), array([ 2.67259244, -3.12666967])] , tabular : [array([-1.4052993 , -3.91919617]), array([-1.4052993 , -3.91919617])]\n",
      "\t\t max_plane value :(-2.951524171927433, -2.951524171927433) ,tabular value : (-3.8432731118917793, -3.8432731118917793)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.4840188379296877, from tabular = -3.0566192341796885\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 0) :[array([ 2.67259244, -3.12666967]), array([ 2.67259244, -3.12666967])] , tabular : [array([-1.4052993 , -3.91919617]), array([-1.4052993 , -3.91919617])]\n",
      "\t\t max_plane value :(-2.951524171927433, -2.951524171927433) ,tabular value : (-3.8432731118917793, -3.8432731118917793)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF cooperative GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-0.737869234179688, -0.737869234179688)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-0.4056927540429687, -0.4056927540429687)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA True 1 \n",
      "\tbelief expansion done, belief space size = 3\n",
      "\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.0625, 9.0625)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.0625, 9.0625)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA True 2 \n",
      "\tbelief expansion done, belief space size = 5\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.319146517092399, 9.319146517092399)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.319146517092399, 9.319146517092399)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA True 3 \n",
      "\tbelief expansion done, belief space size = 7\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.45621447561085, 9.45621447561085)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.420538385930659, 9.420538385930659)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA True 4 \n",
      "\tbelief expansion done, belief space size = 9\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 1 at timestep 1! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.35028323, -11.76647785]), array([ 7.35028323, 11.76647785])] , value =(-7.483658232257739, 7.483658232257739)\n",
      "\treconstructed tabular alpha : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])], value = (-7.830392262259474, 7.830392262259474)  --  belief [0.96979866 0.03020134]  -- DR <decisionRule.DecisionRule object at 0x1445be140>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -5.7817943227469115, from tabular = -6.3049151697613945\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878531, 5.921475730878531) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -3.5307473660274393, from tabular = -3.467370778545482\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878531, 5.921475730878531) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.555069277853157, 9.555069277853157)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.462793061767455, 9.462793061767455)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA True 5 \n",
      "\tbelief expansion done, belief space size = 11\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 4 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 1 at timestep 2! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.35028323, -11.76647785]), array([ 7.35028323, 11.76647785])] , value =(-7.483658232257739, 7.483658232257739)\n",
      "\treconstructed tabular alpha : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])], value = (-7.830392262259474, 7.830392262259474)  --  belief [0.96979866 0.03020134]  -- DR <decisionRule.DecisionRule object at 0x144468910>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -5.7817943227469115, from tabular = -6.3049151697613945\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878531, 5.921475730878531) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -3.5307473660274393, from tabular = -3.467370778545482\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878531, 5.921475730878531) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 3 at timestep 2! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.44116649, -13.08824084]), array([ 7.44116649, 13.08824084])] , value =(-7.446637802496289, 7.446637802496289)\n",
      "\treconstructed tabular alpha : [array([ -7.83051455, -13.1971523 ]), array([ 7.83051455, 13.1971523 ])], value = (-7.835714157889226, 7.835714157889226)  --  belief [9.99031124e-01 9.68876343e-04]  -- DR <decisionRule.DecisionRule object at 0x144394670>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -5.914879102554385, from tabular = -6.46368069407224\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 5) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.915084726906625, 5.915084726906625) ,tabular value : (-6.465184561086681, 6.465184561086681)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878531, 5.921475730878531) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878531, 5.921475730878531) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -12.723413931630411, from tabular = -12.737752237726\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 5) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.915084726906625, 5.915084726906625) ,tabular value : (-6.465184561086681, 6.465184561086681)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878531, 5.921475730878531) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878531, 5.921475730878531) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365407, 6.120505993365407) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 4 at timestep 2! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -9.9326866 , -11.46703623]), array([ 9.9326866 , 11.46703623])] , value =(-11.465549635387903, 11.465549635387903)\n",
      "\treconstructed tabular alpha : [array([-10.39713106, -11.60004075]), array([10.39713106, 11.60004075])], value = (-11.598875279959204, 11.598875279959204)  --  belief [9.68876343e-04 9.99031124e-01]  -- DR <decisionRule.DecisionRule object at 0x1444219f0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -8.921113689095128, from tabular = -9.472777367643959\n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.717515361564804, 10.717515361564804) ,tabular value : (-10.909090909090908, 10.909090909090908)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.717515361564804, 10.717515361564804) ,tabular value : (-10.909090909090908, 10.909090909090908)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 6) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.719203234915758, 10.719203234915758) ,tabular value : (-10.909090909090908, 10.909090909090908)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -10.719257540603248, from tabular = -10.904819658299935\n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.717515361564804, 10.717515361564804) ,tabular value : (-10.909090909090908, 10.909090909090908)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.717515361564804, 10.717515361564804) ,tabular value : (-10.909090909090908, 10.909090909090908)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 6) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.719203234915758, 10.719203234915758) ,tabular value : (-10.909090909090908, 10.909090909090908)  \n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 1 at timestep 1! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.76701331, -11.48865779]), array([ 7.76701331, 11.48865779])] , value =(-7.879411970904325, 7.879411970904325)\n",
      "\treconstructed tabular alpha : [array([ -8.74043501, -10.83970999]), array([ 8.74043501, 10.83970999])], value = (-8.80383593362739, 8.80383593362739)  --  belief [0.96979866 0.03020134]  -- DR <decisionRule.DecisionRule object at 0x144328760>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -6.376457633483311, from tabular = -7.624972585392836\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.83051455, -13.1971523 ]), array([ 7.83051455, 13.1971523 ])]\n",
      "\t\t max_plane value :(-6.5288445820775305, 6.5288445820775305) ,tabular value : (-7.835714157889226, 7.835714157889226)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])]\n",
      "\t\t max_plane value :(-6.698235501041135, 6.698235501041135) ,tabular value : (-7.830392262259474, 7.830392262259474)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])]\n",
      "\t\t max_plane value :(-6.698235501041135, 6.698235501041135) ,tabular value : (-7.830392262259474, 7.830392262259474)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -3.418202391616969, from tabular = -3.2353187111339174\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.83051455, -13.1971523 ]), array([ 7.83051455, 13.1971523 ])]\n",
      "\t\t max_plane value :(-6.5288445820775305, 6.5288445820775305) ,tabular value : (-7.835714157889226, 7.835714157889226)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])]\n",
      "\t\t max_plane value :(-6.698235501041135, 6.698235501041135) ,tabular value : (-7.830392262259474, 7.830392262259474)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])]\n",
      "\t\t max_plane value :(-6.698235501041135, 6.698235501041135) ,tabular value : (-7.830392262259474, 7.830392262259474)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 2 at timestep 1! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.46492883, -11.69004745]), array([ 7.46492883, 11.69004745])] , value =(-11.5624431923312, 11.5624431923312)\n",
      "\treconstructed tabular alpha : [array([ -7.20938036, -11.86041309]), array([ 7.20938036, 11.86041309])], value = (-11.719945662171073, 11.719945662171073)  --  belief [0.03020134 0.96979866]  -- DR <decisionRule.DecisionRule object at 0x1444200d0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.238979417026308, from tabular = -2.2520259664074604\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.0683943 , -11.28773714]), array([ 8.0683943 , 11.28773714])] , tabular : [array([-10.39713106, -11.60004075]), array([10.39713106, 11.60004075])]\n",
      "\t\t max_plane value :(-11.284617991155848, 11.284617991155848) ,tabular value : (-11.598875279959204, 11.598875279959204)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -11.033763050706987, from tabular = -11.285635764041501\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.0683943 , -11.28773714]), array([ 8.0683943 , 11.28773714])] , tabular : [array([-10.39713106, -11.60004075]), array([10.39713106, 11.60004075])]\n",
      "\t\t max_plane value :(-11.284617991155848, 11.284617991155848) ,tabular value : (-11.598875279959204, 11.598875279959204)  \n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -6.8865018 , -12.07566547]), array([ 6.8865018 , 12.07566547])] , value =(-9.481083633207193, 9.481083633207193)\n",
      "\treconstructed tabular alpha : [array([ -7.84039557, -11.43973629]), array([ 7.84039557, 11.43973629])], value = (-9.640065928705845, 9.640065928705845)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x14446ae90>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -5.070965722284575, from tabular = -6.477175353809205\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.77675837, -12.14882775]), array([ 6.77675837, 12.14882775])] , tabular : [array([ -8.74043501, -10.83970999]), array([ 8.74043501, 10.83970999])]\n",
      "\t\t max_plane value :(-6.939002076782338, 6.939002076782338) ,tabular value : (-8.80383593362739, 8.80383593362739)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 1) :[array([ -7.76701331, -11.48865779]), array([ 7.76701331, 11.48865779])] , tabular : [array([ -7.20938036, -11.86041309]), array([ 7.20938036, 11.86041309])]\n",
      "\t\t max_plane value :(-11.376259133135123, 11.376259133135123) ,tabular value : (-11.719945662171073, 11.719945662171073)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -8.573903879103932, from tabular = -8.813041935060648\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.77675837, -12.14882775]), array([ 6.77675837, 12.14882775])] , tabular : [array([ -8.74043501, -10.83970999]), array([ 8.74043501, 10.83970999])]\n",
      "\t\t max_plane value :(-6.939002076782338, 6.939002076782338) ,tabular value : (-8.80383593362739, 8.80383593362739)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 1) :[array([ -7.76701331, -11.48865779]), array([ 7.76701331, 11.48865779])] , tabular : [array([ -7.20938036, -11.86041309]), array([ 7.20938036, 11.86041309])]\n",
      "\t\t max_plane value :(-11.376259133135123, 11.376259133135123) ,tabular value : (-11.719945662171073, 11.719945662171073)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.640065928705845, 9.640065928705845)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.481083633207193, 9.481083633207193)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA False 1 \n",
      "\tbelief expansion done, belief space size = 3\n",
      "\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.0625, 9.0625)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.0625, 9.0625)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA False 2 \n",
      "\tbelief expansion done, belief space size = 5\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.319146517092399, 9.319146517092399)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.319146517092399, 9.319146517092399)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA False 3 \n",
      "\tbelief expansion done, belief space size = 7\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.456214475610846, 9.456214475610846)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.420538385930659, 9.420538385930659)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA False 4 \n",
      "\tbelief expansion done, belief space size = 9\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 1 at timestep 1! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.35028323, -11.76647785]), array([ 7.35028323, 11.76647785])] , value =(-7.4836582322577385, 7.4836582322577385)\n",
      "\treconstructed tabular alpha : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])], value = (-7.8303922622594735, 7.8303922622594735)  --  belief [0.96979866 0.03020134]  -- DR <decisionRule.DecisionRule object at 0x1444f0cd0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -5.781794322746912, from tabular = -6.3049151697613945\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878532, 5.921475730878532) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -3.5307473660274393, from tabular = -3.467370778545482\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878532, 5.921475730878532) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.555069277853159, 9.555069277853159)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.462793061767453, 9.462793061767453)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving zerosum 2generals GAME WITH SOTA False 5 \n",
      "\tbelief expansion done, belief space size = 11\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 4 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 1 at timestep 2! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.35028323, -11.76647785]), array([ 7.35028323, 11.76647785])] , value =(-7.4836582322577385, 7.4836582322577385)\n",
      "\treconstructed tabular alpha : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])], value = (-7.8303922622594735, 7.8303922622594735)  --  belief [0.96979866 0.03020134]  -- DR <decisionRule.DecisionRule object at 0x144945d80>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -5.781794322746912, from tabular = -6.3049151697613945\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878532, 5.921475730878532) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -3.5307473660274393, from tabular = -3.467370778545482\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878532, 5.921475730878532) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 3 at timestep 2! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.44116649, -13.08824084]), array([ 7.44116649, 13.08824084])] , value =(-7.446637802496289, 7.446637802496289)\n",
      "\treconstructed tabular alpha : [array([ -7.83051455, -13.1971523 ]), array([ 7.83051455, 13.1971523 ])], value = (-7.8357141578892255, 7.8357141578892255)  --  belief [9.99031124e-01 9.68876343e-04]  -- DR <decisionRule.DecisionRule object at 0x144655300>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -5.914879102554386, from tabular = -6.463680694072241\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 5) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.915084726906626, 5.915084726906626) ,tabular value : (-6.465184561086682, 6.465184561086682)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878532, 5.921475730878532) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878532, 5.921475730878532) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -12.723413931630411, from tabular = -12.737752237726001\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 5) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.915084726906626, 5.915084726906626) ,tabular value : (-6.465184561086682, 6.465184561086682)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878532, 5.921475730878532) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.46496815, -13.63057325]), array([ 6.46496815, 13.63057325])]\n",
      "\t\t max_plane value :(-5.921475730878532, 5.921475730878532) ,tabular value : (-6.471910738124236, 6.471910738124236)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -5.9148791 , -12.72341393]), array([ 5.9148791 , 12.72341393])] , tabular : [array([ -6.40774776, -12.39483483]), array([ 6.40774776, 12.39483483])]\n",
      "\t\t max_plane value :(-6.120505993365408, 6.120505993365408) ,tabular value : (-6.588565827691755, 6.588565827691755)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 4 at timestep 2! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -9.9326866 , -11.46703623]), array([ 9.9326866 , 11.46703623])] , value =(-11.465549635387903, 11.465549635387903)\n",
      "\treconstructed tabular alpha : [array([-10.39713106, -11.60004075]), array([10.39713106, 11.60004075])], value = (-11.598875279959206, 11.598875279959206)  --  belief [9.68876343e-04 9.99031124e-01]  -- DR <decisionRule.DecisionRule object at 0x1445eb3a0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -8.921113689095128, from tabular = -9.472777367643959\n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.717515361564804, 10.717515361564804) ,tabular value : (-10.90909090909091, 10.90909090909091)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.717515361564804, 10.717515361564804) ,tabular value : (-10.90909090909091, 10.90909090909091)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 6) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.719203234915758, 10.719203234915758) ,tabular value : (-10.90909090909091, 10.90909090909091)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -10.719257540603248, from tabular = -10.904819658299939\n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.717515361564804, 10.717515361564804) ,tabular value : (-10.90909090909091, 10.90909090909091)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.717515361564804, 10.717515361564804) ,tabular value : (-10.90909090909091, 10.90909090909091)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 6) = from max_plane (alpha built on belief_id 2) :[array([ -8.92111369, -10.71925754]), array([ 8.92111369, 10.71925754])] , tabular : [array([-10.90909091, -10.90909091]), array([10.90909091, 10.90909091])]\n",
      "\t\t max_plane value :(-10.719203234915758, 10.719203234915758) ,tabular value : (-10.90909090909091, 10.90909090909091)  \n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 1 at timestep 1! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.76701331, -11.48865779]), array([ 7.76701331, 11.48865779])] , value =(-7.879411970904326, 7.879411970904326)\n",
      "\treconstructed tabular alpha : [array([ -8.74043501, -10.83970999]), array([ 8.74043501, 10.83970999])], value = (-8.80383593362739, 8.80383593362739)  --  belief [0.96979866 0.03020134]  -- DR <decisionRule.DecisionRule object at 0x14461f2b0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -6.376457633483312, from tabular = -7.624972585392836\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.83051455, -13.1971523 ]), array([ 7.83051455, 13.1971523 ])]\n",
      "\t\t max_plane value :(-6.528844582077531, 6.528844582077531) ,tabular value : (-7.8357141578892255, 7.8357141578892255)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])]\n",
      "\t\t max_plane value :(-6.698235501041136, 6.698235501041136) ,tabular value : (-7.8303922622594735, 7.8303922622594735)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])]\n",
      "\t\t max_plane value :(-6.698235501041136, 6.698235501041136) ,tabular value : (-7.8303922622594735, 7.8303922622594735)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -3.418202391616969, from tabular = -3.2353187111339174\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.83051455, -13.1971523 ]), array([ 7.83051455, 13.1971523 ])]\n",
      "\t\t max_plane value :(-6.528844582077531, 6.528844582077531) ,tabular value : (-7.8357141578892255, 7.8357141578892255)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])]\n",
      "\t\t max_plane value :(-6.698235501041136, 6.698235501041136) ,tabular value : (-7.8303922622594735, 7.8303922622594735)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.52323032, -12.31784646]), array([ 6.52323032, 12.31784646])] , tabular : [array([ -7.71539539, -11.52306974]), array([ 7.71539539, 11.52306974])]\n",
      "\t\t max_plane value :(-6.698235501041136, 6.698235501041136) ,tabular value : (-7.8303922622594735, 7.8303922622594735)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 2 at timestep 1! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -7.46492883, -11.69004745]), array([ 7.46492883, 11.69004745])] , value =(-11.562443192331203, 11.562443192331203)\n",
      "\treconstructed tabular alpha : [array([ -7.20938036, -11.86041309]), array([ 7.20938036, 11.86041309])], value = (-11.719945662171074, 11.719945662171074)  --  belief [0.03020134 0.96979866]  -- DR <decisionRule.DecisionRule object at 0x1445bf370>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.238979417026308, from tabular = -2.2520259664074604\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.0683943 , -11.28773714]), array([ 8.0683943 , 11.28773714])] , tabular : [array([-10.39713106, -11.60004075]), array([10.39713106, 11.60004075])]\n",
      "\t\t max_plane value :(-11.28461799115585, 11.28461799115585) ,tabular value : (-11.598875279959206, 11.598875279959206)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -11.033763050706991, from tabular = -11.285635764041501\n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 4) = from max_plane (alpha built on belief_id 2) :[array([ -8.0683943 , -11.28773714]), array([ 8.0683943 , 11.28773714])] , tabular : [array([-10.39713106, -11.60004075]), array([10.39713106, 11.60004075])]\n",
      "\t\t max_plane value :(-11.28461799115585, 11.28461799115585) ,tabular value : (-11.598875279959206, 11.598875279959206)  \n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game zerosum  :: \n",
      "\tReconstructed Max plane alpha:[array([ -6.8865018 , -12.07566547]), array([ 6.8865018 , 12.07566547])] , value =(-9.481083633207195, 9.481083633207195)\n",
      "\treconstructed tabular alpha : [array([ -7.84039557, -11.43973629]), array([ 7.84039557, 11.43973629])], value = (-9.640065928705845, 9.640065928705845)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x1446b9990>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -5.070965722284575, from tabular = -6.477175353809204\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.77675837, -12.14882775]), array([ 6.77675837, 12.14882775])] , tabular : [array([ -8.74043501, -10.83970999]), array([ 8.74043501, 10.83970999])]\n",
      "\t\t max_plane value :(-6.939002076782338, 6.939002076782338) ,tabular value : (-8.80383593362739, 8.80383593362739)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 1) :[array([ -7.76701331, -11.48865779]), array([ 7.76701331, 11.48865779])] , tabular : [array([ -7.20938036, -11.86041309]), array([ 7.20938036, 11.86041309])]\n",
      "\t\t max_plane value :(-11.376259133135123, 11.376259133135123) ,tabular value : (-11.719945662171074, 11.719945662171074)  \n",
      "\n",
      "\tbeta(x=1, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -8.573903879103932, from tabular = -8.81304193506065\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -6.77675837, -12.14882775]), array([ 6.77675837, 12.14882775])] , tabular : [array([ -8.74043501, -10.83970999]), array([ 8.74043501, 10.83970999])]\n",
      "\t\t max_plane value :(-6.939002076782338, 6.939002076782338) ,tabular value : (-8.80383593362739, 8.80383593362739)  \n",
      "\n",
      "\t\talpha from z = 3 , V^t+1(b_next = 2) = from max_plane (alpha built on belief_id 1) :[array([ -7.76701331, -11.48865779]), array([ 7.76701331, 11.48865779])] , tabular : [array([ -7.20938036, -11.86041309]), array([ 7.20938036, 11.86041309])]\n",
      "\t\t max_plane value :(-11.376259133135123, 11.376259133135123) ,tabular value : (-11.719945662171074, 11.719945662171074)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF zerosum GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-9.640065928705845, 9.640065928705845)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-9.481083633207195, 9.481083633207195)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA True 1 \n",
      "\tbelief expansion done, belief space size = 3\n",
      "\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-5.5, 1.5)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-5.5, 1.5)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA True 2 \n",
      "\tbelief expansion done, belief space size = 5\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-5.9605434782608695, 1.5)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-5.9605434782608695, 1.5)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA True 3 \n",
      "\tbelief expansion done, belief space size = 7\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-6.253175815217391, 1.5)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-6.215604891304348, 1.5)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA True 4 \n",
      "\tbelief expansion done, belief space size = 9\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -2.27720325, -10.        ]), array([0., 3.])] , value =(-6.1386016244092625, 1.5)\n",
      "\treconstructed tabular alpha : [array([ -3.17818318, -10.        ]), array([0., 3.])], value = (-6.589091590914461, 1.5)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x144352e30>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -1.2772032488185254, from tabular = -2.178183181828923\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 2) :[array([ -1.71436678, -10.        ]), array([0., 3.])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-1.964604021929435, 0.09060402684563758) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-6.589091590914461, 1.5)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-6.1386016244092625, 1.5)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA True 5 \n",
      "\tbelief expansion done, belief space size = 11\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 4 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 1! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -2.27720325, -10.        ]), array([0., 3.])] , value =(-6.1386016244092625, 1.5)\n",
      "\treconstructed tabular alpha : [array([ -3.17818318, -10.        ]), array([0., 3.])], value = (-6.589091590914461, 1.5)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x1445bfdc0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -1.2772032488185254, from tabular = -2.178183181828923\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 2) :[array([ -1.71436678, -10.        ]), array([0., 3.])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-1.964604021929435, 0.09060402684563758) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 1 at timestep 1! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -3.37650756, -10.        ]), array([0., 3.])] , value =(-3.5765459247063767, 0.09060402684563758)\n",
      "\treconstructed tabular alpha : [array([ -3.91919617, -10.        ]), array([0., 3.])], value = (-4.102844609637166, 0.09060402684563758)  --  belief [0.96979866 0.03020134]  -- DR <decisionRule.DecisionRule object at 0x14431df60>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.3765075625000005, from tabular = -2.919196171875001\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -2.43120978, -10.        ]), array([0., 3.])] , tabular : [array([ -2.99949375, -10.        ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-2.4385430043929675, 0.002906629028013683) ,tabular value : (-3.006276374892347, 0.002906629028013683)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -2.43120978, -10.        ]), array([0., 3.])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-2.6597974066238694, 0.09060402684563758) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -2.43120978, -10.        ]), array([0., 3.])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-2.6597974066238694, 0.09060402684563758) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -2.0994239, -10.       ]), array([0., 3.])] , value =(-6.04971195077357, 1.5)\n",
      "\treconstructed tabular alpha : [array([ -3.86936072, -10.        ]), array([0., 3.])], value = (-6.934680357883758, 1.5)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x127a88640>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -1.0994239015471403, from tabular = -2.8693607157675163\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 2) :[array([ -1.47573678, -10.        ]), array([0., 3.])] , tabular : [array([ -3.91919617, -10.        ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-1.733180971790116, 0.09060402684563758) ,tabular value : (-4.102844609637166, 0.09060402684563758)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA True ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-6.934680357883758, 1.5)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-6.04971195077357, 1.5)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\n",
      "EXTRACTING STACKELBERG POLICIES ... \n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA False 1 \n",
      "\tbelief expansion done, belief space size = 3\n",
      "\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "[{0}, {0, 1, 2}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-5.5, 1.5)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-5.5, 1.5)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA False 2 \n",
      "\tbelief expansion done, belief space size = 5\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-5.9605434782608695, 1.382608695652174)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-5.9605434782608695, 1.382608695652174)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA False 3 \n",
      "\tbelief expansion done, belief space size = 7\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-6.253175815217391, 1.4674239130434783)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-6.215604891304348, 1.4125434782608695)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA False 4 \n",
      "\tbelief expansion done, belief space size = 9\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -2.27734403, -10.        ]), array([-0.24359765,  3.        ])] , value =(-6.138672015139751, 1.378201176242236)\n",
      "\treconstructed tabular alpha : [array([ -3.17818743, -10.        ]), array([-0.02262372,  3.        ])], value = (-6.589093716809007, 1.488688142468944)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x1446288b0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -1.2773440302795032, from tabular = -2.178187433618013\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 2) :[array([ -1.71455575, -10.        ]), array([-0.32697671,  3.        ])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-1.9647872832339823, -0.22649754575013553) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-6.589093716809007, 1.488688142468944)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-6.138672015139751, 1.378201176242236)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\t\t\t Solving stackelberg 2generals GAME WITH SOTA False 5 \n",
      "\tbelief expansion done, belief space size = 11\n",
      "\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "[{0}, {0, 1, 2}, {0, 1, 2, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, set()]\n",
      "iteration : 1\n",
      "\n",
      "========== Backup at timestep 4 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 3 done, verification done ==========\n",
      "\n",
      "========== Backup at timestep 2 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 1! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -2.27734403, -10.        ]), array([-0.24359765,  3.        ])] , value =(-6.138672015139751, 1.378201176242236)\n",
      "\treconstructed tabular alpha : [array([ -3.17818743, -10.        ]), array([-0.02262372,  3.        ])], value = (-6.589093716809007, 1.488688142468944)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x1445d7df0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -1.2773440302795032, from tabular = -2.178187433618013\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 2) :[array([ -1.71455575, -10.        ]), array([-0.32697671,  3.        ])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-1.9647872832339823, -0.22649754575013553) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 1 at timestep 1! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -3.37650756, -10.        ]), array([-0.1709775,  3.       ])] , value =(-3.5765459247063767, -0.07520972315436243)\n",
      "\treconstructed tabular alpha : [array([ -3.91919617, -10.        ]), array([0., 3.])], value = (-4.102844609637166, 0.09060402684563758)  --  belief [0.96979866 0.03020134]  -- DR <decisionRule.DecisionRule object at 0x1446086a0>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -2.3765075625000005, from tabular = -2.919196171875001\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 3) = from max_plane (alpha built on belief_id 0) :[array([ -2.43120978, -10.        ]), array([-0.17491304,  3.        ])] , tabular : [array([ -2.99949375, -10.        ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-2.4385430043929675, -0.17183694534039648) ,tabular value : (-3.006276374892347, 0.002906629028013683)  \n",
      "\n",
      "\t\talpha from z = 1 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -2.43120978, -10.        ]), array([-0.17491304,  3.        ])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-2.6597974066238694, -0.07902640793697112) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "\t\talpha from z = 2 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 0) :[array([ -2.43120978, -10.        ]), array([-0.17491304,  3.        ])] , tabular : [array([ -2.9492625, -10.       ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-2.6597974066238694, -0.07902640793697112) ,tabular value : (-3.1622042365771823, 0.09060402684563758)  \n",
      "\n",
      "========== Backup at timestep 1 done, verification done ==========\n",
      "\n",
      "\n",
      " FOUND DIFFERENCE during backuo of belief ID : 0 at timestep 0! \n",
      "Game stackelberg  :: \n",
      "\tReconstructed Max plane alpha:[array([ -2.09946297, -10.        ]), array([-0.06759835,  3.        ])] , value =(-6.049731484201281, 1.4662008264072206)\n",
      "\treconstructed tabular alpha : [array([ -3.86936298, -10.        ]), array([-0.00849593,  3.        ])], value = (-6.934681489922603, 1.4957520362043866)  --  belief [0.5 0.5]  -- DR <decisionRule.DecisionRule object at 0x144313940>\n",
      "\n",
      "looking into beta vector..\n",
      "\n",
      "agent 0\n",
      "\n",
      "\tbeta(x=0, u=0),\n",
      "\treward  = -1.0 + future component from max_plane = -1.099462968402562, from tabular = -2.8693629798452065\n",
      "\n",
      "\t\talpha from z = 0 , V^t+1(b_next = 1) = from max_plane (alpha built on belief_id 2) :[array([ -1.47578922, -10.        ]), array([-0.09073604,  3.        ])] , tabular : [array([ -3.91919617, -10.        ]), array([0., 3.])]\n",
      "\t\t max_plane value :(-1.733231826802128, 0.0026083404503105534) ,tabular value : (-4.102844609637166, 0.09060402684563758)  \n",
      "\n",
      "========== Backup at timestep 0 done, verification done ==========\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================= END OF stackelberg GAME WITH SOTA False ======================================================================\n",
      "\n",
      "\t\t\tpoint value at initial belief  (-6.934681489922603, 1.4957520362043866)\n",
      "\t\t\talphavectors value at inital belief (V0,V1) : (-6.049731484201281, 1.4662008264072206)\n",
      "\n",
      "\n",
      "==========================================================================================================================================================================\n",
      "\n",
      "EXTRACTING STACKELBERG POLICIES ... \n",
      "calculating stackelberg comparsion matrix...\n",
      "belief od = 0 at timestep 0\n",
      "belief od = 0 at timestep 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gametype</th>\n",
       "      <th>SOTA</th>\n",
       "      <th>horizon</th>\n",
       "      <th>iterations</th>\n",
       "      <th>time</th>\n",
       "      <th>number_of_beliefs</th>\n",
       "      <th>values</th>\n",
       "      <th>tabular value</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.002260923385620117]</td>\n",
       "      <td>3</td>\n",
       "      <td>[(-1.0, -1.0)]</td>\n",
       "      <td>[(-1.0, -1.0)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.004668712615966797]</td>\n",
       "      <td>5</td>\n",
       "      <td>[(0.32125000000000004, 0.32125000000000004)]</td>\n",
       "      <td>[(0.32125000000000004, 0.32125000000000004)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.010259866714477539]</td>\n",
       "      <td>7</td>\n",
       "      <td>[(0.019353125000000082, 0.019353125000000082)]</td>\n",
       "      <td>[(-0.034993749999999935, -0.034993749999999935)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.02063274383544922]</td>\n",
       "      <td>9</td>\n",
       "      <td>[(-0.22703861718750007, -0.22703861718750007)]</td>\n",
       "      <td>[(-0.38709215625000004, -0.38709215625000004)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.039689064025878906]</td>\n",
       "      <td>11</td>\n",
       "      <td>[(-0.4056927540429687, -0.4056927540429687)]</td>\n",
       "      <td>[(-0.737869234179688, -0.737869234179688)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.027376174926757812]</td>\n",
       "      <td>3</td>\n",
       "      <td>[(-1.0, -1.0)]</td>\n",
       "      <td>[(-1.0, -1.0)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.10472488403320312]</td>\n",
       "      <td>5</td>\n",
       "      <td>[(0.32125000000000004, 0.32125000000000004)]</td>\n",
       "      <td>[(0.32125000000000004, 0.32125000000000004)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2507929801940918]</td>\n",
       "      <td>7</td>\n",
       "      <td>[(0.019353125000000082, 0.019353125000000082)]</td>\n",
       "      <td>[(-0.034993749999999935, -0.034993749999999935)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.42410707473754883]</td>\n",
       "      <td>9</td>\n",
       "      <td>[(-0.22703861718750007, -0.22703861718750007)]</td>\n",
       "      <td>[(-0.38709215625000004, -0.38709215625000004)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cooperative</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6653249263763428]</td>\n",
       "      <td>11</td>\n",
       "      <td>[(-0.4056927540429687, -0.4056927540429687)]</td>\n",
       "      <td>[(-0.737869234179688, -0.737869234179688)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.047232866287231445]</td>\n",
       "      <td>3</td>\n",
       "      <td>[(-9.0625, 9.0625)]</td>\n",
       "      <td>[(-9.0625, 9.0625)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1731870174407959]</td>\n",
       "      <td>5</td>\n",
       "      <td>[(-9.319146517092399, 9.319146517092399)]</td>\n",
       "      <td>[(-9.319146517092399, 9.319146517092399)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3740260601043701]</td>\n",
       "      <td>7</td>\n",
       "      <td>[(-9.420538385930659, 9.420538385930659)]</td>\n",
       "      <td>[(-9.45621447561085, 9.45621447561085)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.637883186340332]</td>\n",
       "      <td>9</td>\n",
       "      <td>[(-9.462793061767455, 9.462793061767455)]</td>\n",
       "      <td>[(-9.555069277853157, 9.555069277853157)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.9901371002197266]</td>\n",
       "      <td>11</td>\n",
       "      <td>[(-9.481083633207193, 9.481083633207193)]</td>\n",
       "      <td>[(-9.640065928705845, 9.640065928705845)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04536294937133789]</td>\n",
       "      <td>3</td>\n",
       "      <td>[(-9.0625, 9.0625)]</td>\n",
       "      <td>[(-9.0625, 9.0625)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.12665700912475586]</td>\n",
       "      <td>5</td>\n",
       "      <td>[(-9.319146517092399, 9.319146517092399)]</td>\n",
       "      <td>[(-9.319146517092399, 9.319146517092399)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.31049585342407227]</td>\n",
       "      <td>7</td>\n",
       "      <td>[(-9.420538385930659, 9.420538385930659)]</td>\n",
       "      <td>[(-9.456214475610846, 9.456214475610846)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4734330177307129]</td>\n",
       "      <td>9</td>\n",
       "      <td>[(-9.462793061767453, 9.462793061767453)]</td>\n",
       "      <td>[(-9.555069277853159, 9.555069277853159)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>zerosum</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.7619209289550781]</td>\n",
       "      <td>11</td>\n",
       "      <td>[(-9.481083633207195, 9.481083633207195)]</td>\n",
       "      <td>[(-9.640065928705845, 9.640065928705845)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.02771306037902832]</td>\n",
       "      <td>3</td>\n",
       "      <td>[(-5.5, 1.5)]</td>\n",
       "      <td>[(-5.5, 1.5)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.14974737167358398]</td>\n",
       "      <td>5</td>\n",
       "      <td>[(-5.9605434782608695, 1.5)]</td>\n",
       "      <td>[(-5.9605434782608695, 1.5)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2567710876464844]</td>\n",
       "      <td>7</td>\n",
       "      <td>[(-6.215604891304348, 1.5)]</td>\n",
       "      <td>[(-6.253175815217391, 1.5)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.44080424308776855]</td>\n",
       "      <td>9</td>\n",
       "      <td>[(-6.1386016244092625, 1.5)]</td>\n",
       "      <td>[(-6.589091590914461, 1.5)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>State of the Art</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6927292346954346]</td>\n",
       "      <td>11</td>\n",
       "      <td>[(-6.04971195077357, 1.5)]</td>\n",
       "      <td>[(-6.934680357883758, 1.5)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.027086973190307617]</td>\n",
       "      <td>3</td>\n",
       "      <td>[(-5.5, 1.5)]</td>\n",
       "      <td>[(-5.5, 1.5)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.11787104606628418]</td>\n",
       "      <td>5</td>\n",
       "      <td>[(-5.9605434782608695, 1.382608695652174)]</td>\n",
       "      <td>[(-5.9605434782608695, 1.382608695652174)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2734360694885254]</td>\n",
       "      <td>7</td>\n",
       "      <td>[(-6.215604891304348, 1.4125434782608695)]</td>\n",
       "      <td>[(-6.253175815217391, 1.4674239130434783)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.48478007316589355]</td>\n",
       "      <td>9</td>\n",
       "      <td>[(-6.138672015139751, 1.378201176242236)]</td>\n",
       "      <td>[(-6.589093716809007, 1.488688142468944)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>stackelberg</td>\n",
       "      <td>Stackelberg</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.7194597721099854]</td>\n",
       "      <td>11</td>\n",
       "      <td>[(-6.049731484201281, 1.4662008264072206)]</td>\n",
       "      <td>[(-6.934681489922603, 1.4957520362043866)]</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gametype              SOTA  horizon  iterations  \\\n",
       "0   cooperative  State of the Art        1           1   \n",
       "1   cooperative  State of the Art        2           1   \n",
       "2   cooperative  State of the Art        3           1   \n",
       "3   cooperative  State of the Art        4           1   \n",
       "4   cooperative  State of the Art        5           1   \n",
       "5   cooperative       Stackelberg        1           1   \n",
       "6   cooperative       Stackelberg        2           1   \n",
       "7   cooperative       Stackelberg        3           1   \n",
       "8   cooperative       Stackelberg        4           1   \n",
       "9   cooperative       Stackelberg        5           1   \n",
       "10      zerosum  State of the Art        1           1   \n",
       "11      zerosum  State of the Art        2           1   \n",
       "12      zerosum  State of the Art        3           1   \n",
       "13      zerosum  State of the Art        4           1   \n",
       "14      zerosum  State of the Art        5           1   \n",
       "15      zerosum       Stackelberg        1           1   \n",
       "16      zerosum       Stackelberg        2           1   \n",
       "17      zerosum       Stackelberg        3           1   \n",
       "18      zerosum       Stackelberg        4           1   \n",
       "19      zerosum       Stackelberg        5           1   \n",
       "20  stackelberg  State of the Art        1           1   \n",
       "21  stackelberg  State of the Art        2           1   \n",
       "22  stackelberg  State of the Art        3           1   \n",
       "23  stackelberg  State of the Art        4           1   \n",
       "24  stackelberg  State of the Art        5           1   \n",
       "25  stackelberg       Stackelberg        1           1   \n",
       "26  stackelberg       Stackelberg        2           1   \n",
       "27  stackelberg       Stackelberg        3           1   \n",
       "28  stackelberg       Stackelberg        4           1   \n",
       "29  stackelberg       Stackelberg        5           1   \n",
       "\n",
       "                      time  number_of_beliefs  \\\n",
       "0   [0.002260923385620117]                  3   \n",
       "1   [0.004668712615966797]                  5   \n",
       "2   [0.010259866714477539]                  7   \n",
       "3    [0.02063274383544922]                  9   \n",
       "4   [0.039689064025878906]                 11   \n",
       "5   [0.027376174926757812]                  3   \n",
       "6    [0.10472488403320312]                  5   \n",
       "7     [0.2507929801940918]                  7   \n",
       "8    [0.42410707473754883]                  9   \n",
       "9     [0.6653249263763428]                 11   \n",
       "10  [0.047232866287231445]                  3   \n",
       "11    [0.1731870174407959]                  5   \n",
       "12    [0.3740260601043701]                  7   \n",
       "13     [0.637883186340332]                  9   \n",
       "14    [0.9901371002197266]                 11   \n",
       "15   [0.04536294937133789]                  3   \n",
       "16   [0.12665700912475586]                  5   \n",
       "17   [0.31049585342407227]                  7   \n",
       "18    [0.4734330177307129]                  9   \n",
       "19    [0.7619209289550781]                 11   \n",
       "20   [0.02771306037902832]                  3   \n",
       "21   [0.14974737167358398]                  5   \n",
       "22    [0.2567710876464844]                  7   \n",
       "23   [0.44080424308776855]                  9   \n",
       "24    [0.6927292346954346]                 11   \n",
       "25  [0.027086973190307617]                  3   \n",
       "26   [0.11787104606628418]                  5   \n",
       "27    [0.2734360694885254]                  7   \n",
       "28   [0.48478007316589355]                  9   \n",
       "29    [0.7194597721099854]                 11   \n",
       "\n",
       "                                            values  \\\n",
       "0                                   [(-1.0, -1.0)]   \n",
       "1     [(0.32125000000000004, 0.32125000000000004)]   \n",
       "2   [(0.019353125000000082, 0.019353125000000082)]   \n",
       "3   [(-0.22703861718750007, -0.22703861718750007)]   \n",
       "4     [(-0.4056927540429687, -0.4056927540429687)]   \n",
       "5                                   [(-1.0, -1.0)]   \n",
       "6     [(0.32125000000000004, 0.32125000000000004)]   \n",
       "7   [(0.019353125000000082, 0.019353125000000082)]   \n",
       "8   [(-0.22703861718750007, -0.22703861718750007)]   \n",
       "9     [(-0.4056927540429687, -0.4056927540429687)]   \n",
       "10                             [(-9.0625, 9.0625)]   \n",
       "11       [(-9.319146517092399, 9.319146517092399)]   \n",
       "12       [(-9.420538385930659, 9.420538385930659)]   \n",
       "13       [(-9.462793061767455, 9.462793061767455)]   \n",
       "14       [(-9.481083633207193, 9.481083633207193)]   \n",
       "15                             [(-9.0625, 9.0625)]   \n",
       "16       [(-9.319146517092399, 9.319146517092399)]   \n",
       "17       [(-9.420538385930659, 9.420538385930659)]   \n",
       "18       [(-9.462793061767453, 9.462793061767453)]   \n",
       "19       [(-9.481083633207195, 9.481083633207195)]   \n",
       "20                                   [(-5.5, 1.5)]   \n",
       "21                    [(-5.9605434782608695, 1.5)]   \n",
       "22                     [(-6.215604891304348, 1.5)]   \n",
       "23                    [(-6.1386016244092625, 1.5)]   \n",
       "24                      [(-6.04971195077357, 1.5)]   \n",
       "25                                   [(-5.5, 1.5)]   \n",
       "26      [(-5.9605434782608695, 1.382608695652174)]   \n",
       "27      [(-6.215604891304348, 1.4125434782608695)]   \n",
       "28       [(-6.138672015139751, 1.378201176242236)]   \n",
       "29      [(-6.049731484201281, 1.4662008264072206)]   \n",
       "\n",
       "                                       tabular value       density  \n",
       "0                                     [(-1.0, -1.0)]  1.000000e-07  \n",
       "1       [(0.32125000000000004, 0.32125000000000004)]  1.000000e-07  \n",
       "2   [(-0.034993749999999935, -0.034993749999999935)]  1.000000e-07  \n",
       "3     [(-0.38709215625000004, -0.38709215625000004)]  1.000000e-07  \n",
       "4         [(-0.737869234179688, -0.737869234179688)]  1.000000e-07  \n",
       "5                                     [(-1.0, -1.0)]  1.000000e-07  \n",
       "6       [(0.32125000000000004, 0.32125000000000004)]  1.000000e-07  \n",
       "7   [(-0.034993749999999935, -0.034993749999999935)]  1.000000e-07  \n",
       "8     [(-0.38709215625000004, -0.38709215625000004)]  1.000000e-07  \n",
       "9         [(-0.737869234179688, -0.737869234179688)]  1.000000e-07  \n",
       "10                               [(-9.0625, 9.0625)]  1.000000e-07  \n",
       "11         [(-9.319146517092399, 9.319146517092399)]  1.000000e-07  \n",
       "12           [(-9.45621447561085, 9.45621447561085)]  1.000000e-07  \n",
       "13         [(-9.555069277853157, 9.555069277853157)]  1.000000e-07  \n",
       "14         [(-9.640065928705845, 9.640065928705845)]  1.000000e-07  \n",
       "15                               [(-9.0625, 9.0625)]  1.000000e-07  \n",
       "16         [(-9.319146517092399, 9.319146517092399)]  1.000000e-07  \n",
       "17         [(-9.456214475610846, 9.456214475610846)]  1.000000e-07  \n",
       "18         [(-9.555069277853159, 9.555069277853159)]  1.000000e-07  \n",
       "19         [(-9.640065928705845, 9.640065928705845)]  1.000000e-07  \n",
       "20                                     [(-5.5, 1.5)]  1.000000e-07  \n",
       "21                      [(-5.9605434782608695, 1.5)]  1.000000e-07  \n",
       "22                       [(-6.253175815217391, 1.5)]  1.000000e-07  \n",
       "23                       [(-6.589091590914461, 1.5)]  1.000000e-07  \n",
       "24                       [(-6.934680357883758, 1.5)]  1.000000e-07  \n",
       "25                                     [(-5.5, 1.5)]  1.000000e-07  \n",
       "26        [(-5.9605434782608695, 1.382608695652174)]  1.000000e-07  \n",
       "27        [(-6.253175815217391, 1.4674239130434783)]  1.000000e-07  \n",
       "28         [(-6.589093716809007, 1.488688142468944)]  1.000000e-07  \n",
       "29        [(-6.934681489922603, 1.4957520362043866)]  1.000000e-07  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database, matrix , policy= experiment.run_experiments(num_iterations,density=0.0000001)\n",
    "pd.DataFrame(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cooperative': {True: [<policyTree.PolicyTree at 0x1055e3430>,\n",
       "   <policyTree.PolicyTree at 0x127a63e50>],\n",
       "  False: [<policyTree.PolicyTree at 0x1445e8640>,\n",
       "   <policyTree.PolicyTree at 0x1443d8460>]},\n",
       " 'stackelberg': {True: [<policyTree.PolicyTree at 0x1055e2320>,\n",
       "   <policyTree.PolicyTree at 0x1443ed9c0>],\n",
       "  False: [<policyTree.PolicyTree at 0x1443c4400>,\n",
       "   <policyTree.PolicyTree at 0x1444f3b20>]},\n",
       " 'zerosum': {True: [<policyTree.PolicyTree at 0x12772b220>,\n",
       "   <policyTree.PolicyTree at 0x144325300>],\n",
       "  False: [<policyTree.PolicyTree at 0x1273c4e20>,\n",
       "   <policyTree.PolicyTree at 0x1449695d0>]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:183: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  data = self.database[self.database[\"gametype\"]==gametype][self.database[\"horizon\"]==self.planning_horizon]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplots\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m experiment\u001b[38;5;241m.\u001b[39mhorizon_value_plot()\n",
      "File \u001b[0;32m~/Documents/GitHub/Thesis/experimentFunctions.py:184\u001b[0m, in \u001b[0;36mExperiment.plots\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m,gametype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcooperative\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstackelberg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzerosum\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    183\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgametype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mgametype][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizon\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplanning_horizon]\n\u001b[0;32m--> 184\u001b[0m     x \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sota \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStackelberg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState of the Art\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    186\u001b[0m         y \u001b[38;5;241m=\u001b[39m [values[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m values \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m][data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOTA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39msota])[\u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.plots()\n",
    "experiment.horizon_value_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n",
      "/Users/izzikampono/Documents/GitHub/Thesis/experimentFunctions.py:213: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  current_data = self.database[self.database[\"SOTA\"]==SOTA][self.database[\"horizon\"]==horizon+1][self.database[\"gametype\"]==gametype]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cooperative':           State of the Art                                                    \\\n",
       "                       time                                   value iteration   \n",
       " relay4(0)         0.933659  (7.409999999999998, 7.409999999999998)         3   \n",
       " relay4(1)         0.923161  (7.409999999999998, 7.409999999999998)         3   \n",
       " relay4(2)         0.922331  (7.409999999999998, 7.409999999999998)         3   \n",
       " relay4(3)         0.901461  (7.409999999999998, 7.409999999999998)         3   \n",
       " \n",
       "                PBVI                                                    \n",
       "                time                                   value iteration  \n",
       " relay4(0)  4.709343  (7.409999999999998, 7.409999999999998)         3  \n",
       " relay4(1)  4.689476  (7.409999999999998, 7.409999999999998)         3  \n",
       " relay4(2)  4.711464  (7.409999999999998, 7.409999999999998)         3  \n",
       " relay4(3)  4.664090  (7.409999999999998, 7.409999999999998)         3  ,\n",
       " 'zerosum':           State of the Art                               PBVI  \\\n",
       "                       time          value iteration      time   \n",
       " relay4(0)         7.010297  (-50.0, 50.0)         3  5.155210   \n",
       " relay4(1)         6.834620  (-50.0, 50.0)         3  5.227437   \n",
       " relay4(2)         7.188694  (-50.0, 50.0)         3  5.261423   \n",
       " relay4(3)         6.837514  (-50.0, 50.0)         3  5.117113   \n",
       " \n",
       "                                                               \n",
       "                                              value iteration  \n",
       " relay4(0)  (-51.85878962536023, 51.85878962536023)         3  \n",
       " relay4(1)  (-51.85878962536023, 51.85878962536023)         3  \n",
       " relay4(2)  (-51.85878962536023, 51.85878962536023)         3  \n",
       " relay4(3)  (-51.85878962536023, 51.85878962536023)         3  ,\n",
       " 'stackelberg':           State of the Art                              PBVI  \\\n",
       "                       time         value iteration      time   \n",
       " relay4(0)         5.752386  (-2.5, 44.0)         3  5.350692   \n",
       " relay4(1)         5.714477  (-2.5, 44.0)         3  5.325633   \n",
       " relay4(2)         5.748621  (-2.5, 44.0)         3  5.426654   \n",
       " relay4(3)         5.814187  (-2.5, 44.0)         3  5.472348   \n",
       " \n",
       "                                                                \n",
       "                                               value iteration  \n",
       " relay4(0)  (-2.8937548335072054, 83.92397463208766)         3  \n",
       " relay4(1)  (-2.8937548335072054, 83.92397463208766)         3  \n",
       " relay4(2)  (-2.8937548335072054, 83.92397463208766)         3  \n",
       " relay4(3)  (-2.8937548335072054, 83.92397463208766)         3  }"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.generate_summary_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
